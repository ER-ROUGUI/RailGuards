{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2U6-RcNVXpSP",
   "metadata": {
    "id": "2U6-RcNVXpSP"
   },
   "source": [
    "# Data Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791292f6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "791292f6",
    "outputId": "c1bc410d-514d-4465-9e75-ae2d8815a639"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv(\"MetroPT3.csv\")\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7uLEBO473Bll",
   "metadata": {
    "id": "7uLEBO473Bll"
   },
   "source": [
    "# Data Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i_iR7xMJVv-i",
   "metadata": {
    "id": "i_iR7xMJVv-i"
   },
   "source": [
    "The MetroPT-3 dataset was designed to facilitate the development of predictive maintenance, anomaly detection, and remaining useful life (RUL) prediction models for train compressors using deep learning and machine learning techniques. It comprises multivariate time series data collected from various analog and digital sensors installed on a train compressor. The data, recorded between February and August 2020, includes 15 signals such as pressures, motor current, oil temperature, and electrical signals from air intake valves. This dataset is suitable for incremental training and contains no sensitive information. Data preprocessing involves segmentation, normalization, and feature extraction. While the dataset itself is unlabeled, failure reports provided by the company are available to evaluate the performance of anomaly detection, failure prediction, and RUL estimation algorithms. Notably, the dataset does not contain any missing values.\n",
    "\n",
    "\n",
    "the link : https://archive.ics.uci.edu/dataset/791/metropt+3+dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tlgQsE5x3fEE",
   "metadata": {
    "id": "tlgQsE5x3fEE"
   },
   "source": [
    "## Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0b8b88",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8f0b8b88",
    "outputId": "5b149794-596b-4525-883d-3e218668cb76"
   },
   "outputs": [],
   "source": [
    "print(data.describe().round(2))\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d44ba87-b80a-41fd-982b-061b6cdd81bb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1d44ba87-b80a-41fd-982b-061b6cdd81bb",
    "outputId": "560e566d-89de-4df0-ff59-3fe32fcd690e"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hI6sjTAB92g_",
   "metadata": {
    "id": "hI6sjTAB92g_"
   },
   "source": [
    "## Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JAQC3yq4-FOa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JAQC3yq4-FOa",
    "outputId": "c86f8d17-72ac-448c-8ee3-40225240d371"
   },
   "outputs": [],
   "source": [
    "print(data.columns)\n",
    "data = data.drop('Unnamed: 0', axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db151043",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb2efc4-bd6e-4501-a39b-6f7ef141c8a1",
   "metadata": {
    "id": "3eb2efc4-bd6e-4501-a39b-6f7ef141c8a1"
   },
   "source": [
    "## Convert the timestamp collumn into pandas.DateTime data type standarzation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3355db20",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42c2fe5-79e9-4810-977c-ae7485ea5b52",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d42c2fe5-79e9-4810-977c-ae7485ea5b52",
    "outputId": "480f151c-875c-4011-8725-a8dbdc2c596e"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "#Check the current type of timestamp\n",
    "print(f\"Current type of timestamp is {type(data.timestamp[0])}\")\n",
    "\n",
    "#Convert timestamp to pandas.DateTime ISO 8601\n",
    "data['timestamp'] = data['timestamp'].apply(pd.to_datetime, format = \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "#Re-check the type\n",
    "print(f\"Current type of timestamp is {type(data.timestamp[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d71d0fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5d71d0fd",
    "outputId": "da58eaef-12e4-4444-86d7-6e4b654c45bd"
   },
   "outputs": [],
   "source": [
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bca1be5-5d0e-4317-a375-dedbe9168c00",
   "metadata": {
    "id": "5bca1be5-5d0e-4317-a375-dedbe9168c00"
   },
   "source": [
    "## Add a label feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g3txUZmdXZU8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g3txUZmdXZU8",
    "outputId": "5aaa3394-881e-4869-eb84-855a86e51624"
   },
   "outputs": [],
   "source": [
    "#Create a new column for target variable called status, indicate the equipment has deficiencies and need to be maintained\n",
    "# status = 0; system ups and running no No Failure\n",
    "# status = 1; system downs and needs recovering ## Failure\n",
    "labeled_data = data.copy()\n",
    "labeled_data['status'] = 0\n",
    "print(labeled_data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab649eb6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ab649eb6",
    "outputId": "a3f95a09-2407-4dfe-9941-e7802dba6193"
   },
   "outputs": [],
   "source": [
    "def to_datetime(xs):\n",
    "  result = []\n",
    "  format =  \"%Y-%m-%d %H:%M:%S\"\n",
    "  for x in xs:\n",
    "    result.append(pd.to_datetime(x, format = format))\n",
    "  return result\n",
    "\n",
    "\n",
    "failure_start_time = to_datetime([\"2020-04-18 00:00:00\", \"2020-05-29 23:30:00\", \"2020-06-05 10:00:00\", \"2020-07-15 14:30:00\"] )\n",
    "failure_end_time   = to_datetime([\"2020-04-18 23:59:00\", \"2020-05-30 06:00:00\", \"2020-06-07 14:30:00\", \"2020-07-15 19:00:00\"] )\n",
    "\n",
    "print(failure_start_time,\"\\n\", failure_end_time[0].minute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KqHsjSKPyFsT",
   "metadata": {
    "id": "KqHsjSKPyFsT"
   },
   "outputs": [],
   "source": [
    "def in_between(x, start, end):\n",
    "\n",
    "  start_con = x >= start\n",
    "  end_con = x<= end\n",
    "\n",
    "  inbetween_con = start_con and end_con\n",
    "  if inbetween_con:\n",
    "    return 1\n",
    "  else:\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oig21csoXP3w",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oig21csoXP3w",
    "outputId": "ddaeae7d-d645-488d-e33d-c13da88cc78e"
   },
   "outputs": [],
   "source": [
    "failure_indx = []\n",
    "for i, (start_time, end_time) in enumerate(zip(failure_start_time, failure_end_time)):\n",
    "  mask = labeled_data['timestamp'].apply(in_between, start = start_time, end = end_time)\n",
    "  indx = labeled_data.index[mask == True].tolist()\n",
    "  failure_indx += indx\n",
    "\n",
    "\n",
    "print(f\" Found {len(failure_indx)} samples representing failure state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27kdTKUCR_S",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b27kdTKUCR_S",
    "outputId": "f13398b5-d843-4be6-a993-969dc4204fc4"
   },
   "outputs": [],
   "source": [
    "#Set the sample with the timestamp falled between the failure time to 1\n",
    "# labeled_data['status'].iloc[failure_indx] = 1\n",
    "labeled_data.loc[failure_indx, 'status'] = 1\n",
    "print(labeled_data['status'].value_counts())\n",
    "print(f\"We have {labeled_data['status'][labeled_data['status']==1].count()} positve samples\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TROhLWInqqGr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TROhLWInqqGr",
    "outputId": "69ab8657-212c-401b-ae32-5722b74dee5d"
   },
   "outputs": [],
   "source": [
    "print(f\"Example of Failure state \\n {labeled_data[labeled_data['status']==1].head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "A8iLUYDTpjfI",
   "metadata": {
    "id": "A8iLUYDTpjfI"
   },
   "source": [
    "## splite the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sEyrVfNUsy98",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sEyrVfNUsy98",
    "outputId": "b92e948b-2d88-4c4b-b223-f35a89479d8d"
   },
   "outputs": [],
   "source": [
    "#Seperate Positive samples and Negative sample\n",
    "pos_data = labeled_data[labeled_data['status'] == 1]\n",
    "neg_data = labeled_data[labeled_data['status'] == 0]\n",
    "\n",
    "#Print out the info of 2 dataset\n",
    "print(f\"Positive dataset\\n {pos_data.info()}\\n\")\n",
    "print(f\"Negative dataset\\n {neg_data.info()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "U9kT3KI0tbCz",
   "metadata": {
    "id": "U9kT3KI0tbCz"
   },
   "source": [
    "As we can see, we have around 30K postive samples and 1500K negative sample. This indicates highly imbalanced dataset. Thus, we have to subsample the negative class to balance the training data. To achive this, we will randomly sample 30K negative sample from the set of 1500K sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6_vdKBfat9fH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6_vdKBfat9fH",
    "outputId": "b5b61cdf-5878-48dc-c767-a884f24dbce4"
   },
   "outputs": [],
   "source": [
    "n_positives = int(pos_data['status'].count())\n",
    "sub_neg_data = neg_data.sample(n_positives, random_state = 42)\n",
    "print(f\"Negative dataset after subsampling {sub_neg_data.info()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yQq9o7quu_ei",
   "metadata": {
    "id": "yQq9o7quu_ei"
   },
   "source": [
    "Now, we merge the postive set and negative set into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_Nmrpj12vIDK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Nmrpj12vIDK",
    "outputId": "b6e0ec55-adc9-4a2b-e6f8-0a62297361d9"
   },
   "outputs": [],
   "source": [
    "merged_data = pd.concat([pos_data, sub_neg_data], axis = 0)\n",
    "print(f\"Merged dataset\\n\")\n",
    "merged_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c5a153-7fb2-4b7d-86d9-a41322313752",
   "metadata": {
    "id": "18c5a153-7fb2-4b7d-86d9-a41322313752"
   },
   "source": [
    "## Valeurs abirrantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6abdcd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7a6abdcd",
    "outputId": "9c9de07f-dfdf-4983-f929-a454d9d21260"
   },
   "outputs": [],
   "source": [
    "def investigate_outliers(data, c):\n",
    "    q1 = data[c].quantile(0.25)\n",
    "    q3 = data[c].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    ll = q1 - 1.5*iqr\n",
    "    ul = q3 + 1.5*iqr\n",
    "\n",
    "    num_outliers = data[data[c] < ll][c].count()  + data[data[c] > ul][c].count()\n",
    "    if num_outliers>0:\n",
    "        print(f\"Found {num_outliers} oulier(s) for feature {c}\")\n",
    "    return {'col': c, 'n_outliers': num_outliers, 'll': ll, 'ul': ul, 'q1': q1, 'q3':q3}\n",
    "\n",
    "print(\"\\nDropping outliers ...\\n\")\n",
    "clean_data = merged_data.copy()\n",
    "for i in range(5):\n",
    "  for c in clean_data.columns:\n",
    "      if c not in [\"Unnamed: 0\",\"timestamp\"]:\n",
    "          cue = investigate_outliers(clean_data, c)\n",
    "          if cue[\"n_outliers\"] > 0 and (cue[\"q1\"]!= cue[\"q3\"]):\n",
    "              print(f\"Droping {cue['n_outliers']} from column {c}\")\n",
    "              clean_data = clean_data[clean_data[c]> cue[\"ll\"]]\n",
    "              clean_data = clean_data[clean_data[c]< cue[\"ul\"]]\n",
    "              print(f\"{clean_data.shape[0]} samples left\\n\")\n",
    "          elif (cue[\"q1\"]== cue[\"q3\"]):\n",
    "              print(\"Skipping .. data has Q1 equals to Q3\")\n",
    "              print(f\"{clean_data.shape[0]} rows left\\n\")\n",
    "\n",
    "\n",
    "print(\"\\nDropping Completed ...\\n\")\n",
    "#Recheck data\n",
    "for c in clean_data.columns:\n",
    "    if c not in [\"Unnamed: 0\",\"timestamp\",\"COMP\", 'status']:\n",
    "        cue = investigate_outliers(clean_data, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vq_Xju3HwQU-",
   "metadata": {
    "id": "vq_Xju3HwQU-"
   },
   "outputs": [],
   "source": [
    "#Investigate the columns with the binary values\n",
    "binary_cols = ['LPS', 'Pressure_switch', 'Oil_level', 'Caudal_impulses']\n",
    "#Ensure the the binary data is binary\n",
    "clean_data[binary_cols] = clean_data[binary_cols].apply(np.round)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AcoWfkxg3US9",
   "metadata": {
    "id": "AcoWfkxg3US9"
   },
   "source": [
    "## III. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiQbhtDt6bAB",
   "metadata": {
    "id": "fiQbhtDt6bAB"
   },
   "source": [
    "### 1) Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OA6ITWCb7A8y",
   "metadata": {
    "id": "OA6ITWCb7A8y"
   },
   "source": [
    "Describing the correlation between the features, the values closer to 1 or -1 represent a stronger relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i9T0eAlo6-zg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 606
    },
    "id": "i9T0eAlo6-zg",
    "outputId": "aa9b7264-a06b-4617-88fb-1ac0a52b42e0"
   },
   "outputs": [],
   "source": [
    "clean_data.corr().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4AC8dXfm7IhS",
   "metadata": {
    "id": "4AC8dXfm7IhS"
   },
   "source": [
    "We can see that our target variable \"status\" has high correlation with TP2, H1, DV_pressure, Oil_temparature, Motor_current, COMP, DV_electric and MPG.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2qVh_sbv6fm0",
   "metadata": {
    "id": "2qVh_sbv6fm0"
   },
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2PO54CY7nwo",
   "metadata": {
    "id": "f2PO54CY7nwo"
   },
   "source": [
    "Below shows a Heat map,which can be used to analyse trends, from the below heat map you can see the trends in correlation of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aQrZeZUl7rBm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 610
    },
    "id": "aQrZeZUl7rBm",
    "outputId": "65c08aac-44e7-414c-aebc-5db06f84b7f6"
   },
   "outputs": [],
   "source": [
    "sns.heatmap(clean_data.corr(),annot=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hO6oX3xs8crF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 82
    },
    "id": "hO6oX3xs8crF",
    "outputId": "890a7941-e6f0-4623-8d7f-61c8ab593db7"
   },
   "outputs": [],
   "source": [
    "sns.pairplot(clean_data,  y_vars = ['status'] , plot_kws=  {'alpha' : 0.1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9-5BlC1J-hyP",
   "metadata": {
    "id": "9-5BlC1J-hyP"
   },
   "source": [
    "Drawing box plot to find outliers, I plot it on scale data so it is easier to visualize different features' range.\n",
    "As we can see our preprocessing function work perfectly that leaves no outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbe88db",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.to_csv('RailGuadrs_Clean_Data.csv')\n",
    "np.savez(\"RailGuadrs_Clean_Data.npz\", clean_data.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5967a34",
   "metadata": {},
   "source": [
    "# Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345dacd5",
   "metadata": {},
   "source": [
    "## LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b380bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_regression = pd.read_csv('RailGuadrs_Clean_Data.csv')\n",
    "X = data_regression.iloc[:, 2:-1]\n",
    "y = data_regression.iloc[:, -1]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f31d383",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2d317e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "print(X_train.shape, X_test.shape)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639c096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train) \n",
    "\n",
    "model_regression = LogisticRegression()\n",
    "model_regression.fit(X_train_scaled, y_train)\n",
    "print(f\"Number of iterations: {model_regression.n_iter_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb9ce49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.columns)\n",
    "print(f\"the coef of the model are : {model_regression.coef_}\")\n",
    "print(f\"the intercept is : {model_regression.intercept_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75270a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdb44dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_regression.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22607d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = ConfusionMatrixDisplay.from_predictions(y_test, y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2a76f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'the accuracy is {accuracy_score(y_test, y_pred)}')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7852ca2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = model_regression.predict_proba(X_test)[:, 1]\n",
    "for threshold in [0.2, 0.4, 0.6, 0.8]:\n",
    "    y_pred_threshold = (y_prob > threshold).astype(int)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_threshold)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label='Threshold = {:.1f} (AUC = {:.8f})'.format(threshold, roc_auc))\n",
    "    #print(threshold, format(roc_auc))\n",
    "# Plot the \"Random\" line as a dashed line from (0,0) to (1,1) for reference\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "\n",
    "# Label axes and set the title for the plot\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR) or Sensitivity')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve with Different Thresholds')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87704362",
   "metadata": {},
   "source": [
    "## Predict with LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e29c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: new sample based on the features\n",
    "new_sample = {\n",
    "    'timestamp': [193.293044], \n",
    "    'TP2': [-0.018],\n",
    "    'TP3': [8.248],\n",
    "    'H1': [8.238],\n",
    "    'DV_pressure': [-0.024],\n",
    "    'Reservoirs': [8.248],\n",
    "    'Oil_temperature': [49.450],\n",
    "    'Motor_current': [0.0400],\n",
    "    'COMP': [1.0],\n",
    "    'DV_eletric': [0.0],\n",
    "    'Towers': [1.0],\n",
    "    'MPG': [1.0],\n",
    "    'LPS': [0.0],\n",
    "    'Pressure_switch': [1.0],\n",
    "    'Oil_level': [1.0],\n",
    "    'Caudal_impulses': [1.0]\n",
    "}\n",
    "\n",
    "new_df = pd.DataFrame(new_sample)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9833e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_feature_names = X_train.columns\n",
    "new_df = new_df[training_feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaab558",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_scaled = scaler.transform(new_df)\n",
    "\n",
    "# Predict the class and probability\n",
    "y_pred = model_regression.predict(new_scaled)\n",
    "y_pred_prob = model_regression.predict_proba(new_scaled)[:, 1]\n",
    "\n",
    "print(f\"Prediction (Class): {y_pred}\")\n",
    "print(f\"Probability of class 1: {y_pred_prob}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5b794b",
   "metadata": {},
   "source": [
    "# Deploy the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114bb5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained logistic regression model\n",
    "joblib.dump(model_regression, 'logistic_regression_model.pkl')\n",
    "\n",
    "# Save the scaler\n",
    "joblib.dump(scaler, 'scaler_logistic.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a354bad8",
   "metadata": {},
   "source": [
    "# Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2a0161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB, BernoulliNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947daeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('RailGuadrs_Clean_Data.csv')\n",
    "\n",
    "\n",
    "# Features and target\n",
    "features = ['timestamp', 'TP2', 'TP3', 'H1', 'DV_pressure', 'Reservoirs',\n",
    "            'Oil_temperature', 'Motor_current', 'COMP', 'DV_eletric',\n",
    "            'Towers', 'MPG', 'LPS', 'Pressure_switch', 'Oil_level', 'Caudal_impulses']\n",
    "target = 'status'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb94307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp to a scaled numeric value\n",
    "dataset['timestamp'] = pd.to_datetime(dataset['timestamp'], errors='coerce')\n",
    "dataset['timestamp'] = (dataset['timestamp'] - dataset['timestamp'].min()) / np.timedelta64(1, 'D')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d8baf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[features]\n",
    "y = dataset[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46dcc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize classifiers\n",
    "gaussian_classifier = GaussianNB()\n",
    "bernoulli_classifier = BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaa4a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultinomialNB requires non-negative features, use MinMaxScaler for preprocessing\n",
    "multinomial_classifier = Pipeline([\n",
    "    ('Normalize', MinMaxScaler()),\n",
    "    ('MultinomialNB', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9ac539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the models\n",
    "gaussian_classifier.fit(X_train, y_train)\n",
    "multinomial_classifier.fit(X_train, y_train)\n",
    "bernoulli_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b48dcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each model\n",
    "for name, model in [\n",
    "    (\"GaussianNB\", gaussian_classifier),\n",
    "    (\"MultinomialNB\", multinomial_classifier),\n",
    "    (\"BernoulliNB\", bernoulli_classifier)\n",
    "]:\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"\\n{name} Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    ConfusionMatrixDisplay.from_predictions(y_test, y_pred).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a9917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample = {\n",
    "    'timestamp': [1.5],  # Replace with a realistic scaled timestamp\n",
    "    'TP2': [-0.018],\n",
    "    'TP3': [8.248],\n",
    "    'H1': [8.238],\n",
    "    'DV_pressure': [-0.024],\n",
    "    'Reservoirs': [8.248],\n",
    "    'Oil_temperature': [49.450],\n",
    "    'Motor_current': [0.0400],\n",
    "    'COMP': [1.0],\n",
    "    'DV_eletric': [0.0],\n",
    "    'Towers': [1.0],\n",
    "    'MPG': [1.0],\n",
    "    'LPS': [0.0],\n",
    "    'Pressure_switch': [1.0],\n",
    "    'Oil_level': [1.0],\n",
    "    'Caudal_impulses': [1.0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096ae01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "new_df = pd.DataFrame(new_sample)\n",
    "\n",
    "# Ensure column order matches training data\n",
    "training_feature_names = [\n",
    "    'timestamp', 'TP2', 'TP3', 'H1', 'DV_pressure', 'Reservoirs',\n",
    "    'Oil_temperature', 'Motor_current', 'COMP', 'DV_eletric',\n",
    "    'Towers', 'MPG', 'LPS', 'Pressure_switch', 'Oil_level', 'Caudal_impulses'\n",
    "]\n",
    "new_df = new_df[training_feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63168077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the class and probability for the new sample\n",
    "y_pred_gaussian = gaussian_classifier.predict(new_df)\n",
    "y_pred_gaussian_prob = gaussian_classifier.predict_proba(new_df)[:, 1]\n",
    "\n",
    "# Map the prediction to class labels\n",
    "class_labels = {0: \"Not Failure\", 1: \"Failure\"}\n",
    "predicted_label = [class_labels[label] for label in y_pred_gaussian]\n",
    "\n",
    "# Output the results\n",
    "print(f\"Prediction (Class): {predicted_label}\")\n",
    "print(f\"Probability of Failure: {y_pred_gaussian_prob}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4943784",
   "metadata": {},
   "source": [
    "## Deploy the Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac570cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(gaussian_classifier, 'gaussian_nb_model.pkl')\n",
    "joblib.dump(multinomial_classifier, 'multinomial_nb_model.pkl')\n",
    "joblib.dump(bernoulli_classifier, 'bernoulli_nb_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc35ef5",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200f0a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0271d69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('RailGuadrs_Clean_Data.csv')\n",
    "\n",
    "# Convert timestamp to numerical values\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "df['timestamp'] = (df['timestamp'] - df['timestamp'].min()) / pd.Timedelta(days=1)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# Display the first rows of the cleaned dataset\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef549f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and target\n",
    "X = df.drop(columns=['status'])  # All columns except 'status'\n",
    "y = df['status']  # Target column\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training set size:\", X_train.shape)\n",
    "print(\"Test set size:\", X_test.shape)\n",
    "\n",
    "# Normalize features\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a513e63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize XGBoost model\n",
    "xgb_model = XGBClassifier(\n",
    "    max_depth=6,\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    objective='binary:logistic',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Model training completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cadfc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Display confusion matrix\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e67566f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(xgb_model, 'xgb_model.pkl')\n",
    "\n",
    "# Save the scaler\n",
    "joblib.dump(scaler, 'scaler_xgb.pkl')\n",
    "\n",
    "print(\"Model and scaler saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267f1a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "xgb_model = joblib.load('xgb_model.pkl')\n",
    "scaler = joblib.load('scaler_xgb.pkl')\n",
    "\n",
    "\n",
    "# Example new data\n",
    "new_sample = {\n",
    "    'timestamp': [1.5],  # Replace with a realistic scaled timestamp\n",
    "    'TP2': [30.018],\n",
    "    'TP3': [8.248],\n",
    "    'H1': [25.238],\n",
    "    'DV_pressure': [5.024],\n",
    "    'Reservoirs': [-88.248],\n",
    "    'Oil_temperature': [80.450],\n",
    "    'Motor_current': [10.0400],\n",
    "    'COMP': [0.0],\n",
    "    'DV_eletric': [0.0],\n",
    "    'Towers': [0.0],\n",
    "    'MPG': [0.0],\n",
    "    'LPS': [1.0],\n",
    "    'Pressure_switch': [0.0],\n",
    "    'Oil_level': [1.0],\n",
    "    'Caudal_impulses': [1.0]\n",
    "}\n",
    "\n",
    "\n",
    "# Convert to DataFrame\n",
    "new_df = pd.DataFrame(new_sample)\n",
    "\n",
    "# Scale the new data\n",
    "new_df_scaled = scaler.transform(new_df)\n",
    "\n",
    "# Predict\n",
    "prediction = xgb_model.predict(new_df_scaled)\n",
    "print(\"Prediction (Class):\", \"Failure\" if prediction[0] == 1 else \"Normal\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073fec92",
   "metadata": {},
   "source": [
    "# DeepLearning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9b652f",
   "metadata": {},
   "source": [
    "##  dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d026785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('RailGuadrs_Clean_Data.csv')\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(columns=['Unnamed: 0', 'timestamp'])\n",
    "\n",
    "# Drop any remaining NaN values\n",
    "df = df.dropna()\n",
    "\n",
    "# Features and target\n",
    "X = df.drop(columns=['status'])  # All columns except 'status'\n",
    "y = df['status']  # Target column\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff70ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('RailGuadrs_Clean_Data.csv')\n",
    "\n",
    "# Convert timestamp to numerical values\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "df['timestamp'] = (df['timestamp'] - df['timestamp'].min()) / pd.Timedelta(days=1)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# Features and target\n",
    "X = df.drop(columns=['status'])\n",
    "y = df['status']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize features\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde4f5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('RailGuadrs_Clean_Data.csv')\n",
    "\n",
    "# Convert timestamp to numerical values\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "df['timestamp'] = (df['timestamp'] - df['timestamp'].min()) / pd.Timedelta(days=1)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# Features and target\n",
    "X = df.drop(columns=['status'])\n",
    "y = df['status']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize features\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fa2edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3049693",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce7a239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('neural_net_model.h5')\n",
    "\n",
    "# Save the scaler\n",
    "import joblib\n",
    "joblib.dump(scaler, 'scaler_net.pkl')\n",
    "\n",
    "print(\"Model and scaler saved successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "tlgQsE5x3fEE",
    "3eb2efc4-bd6e-4501-a39b-6f7ef141c8a1",
    "5bca1be5-5d0e-4317-a375-dedbe9168c00",
    "A8iLUYDTpjfI",
    "18c5a153-7fb2-4b7d-86d9-a41322313752"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
